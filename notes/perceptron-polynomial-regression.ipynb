{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron (Online) Polynomial Regression\n",
    "\n",
    "We will describe a procedure in which one can perform online polynomial\n",
    "regression without exploding the values into polynomial features\n",
    "(e.g. using `sklearn`'s `PolynomialFeatures`).\n",
    "This should lessen the memory burden of the polynomial regression,\n",
    "notably when fitting very high order polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network as an Equation\n",
    "\n",
    "The implementation of a fully connected neural network is achieved\n",
    "through the matrix multiplication of the input vector with a matrix\n",
    "of weights for a network layer.  The bias terms are added after\n",
    "the multiplication and the activation function is performed on the\n",
    "result.  The procedure is repeated for every layer in the network.\n",
    "\n",
    "For example, a neural network with two input nodes (annotated as $i$),\n",
    "three nodes in the hidden layer ($h$) and two output nodes ($o$);\n",
    "will have the following matrices and bias terms:\n",
    "\n",
    "$$\n",
    "L_1 =\n",
    "\\begin{bmatrix}\n",
    "    w_{hi11} & w_{hi12} \\\\\n",
    "    w_{hi21} & w_{hi22} \\\\\n",
    "    w_{hi31} & w_{hi32} \\\\\n",
    "\\end{bmatrix}\n",
    "L_{1bias} =\n",
    "\\begin{bmatrix}\n",
    "    w_{h1} \\\\\n",
    "    w_{h2} \\\\\n",
    "    w_{h3} \\\\\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "L_2 =\n",
    "\\begin{bmatrix}\n",
    "    w_{oh11} & w_{oh12} & w_{oh13} \\\\\n",
    "    w_{oh21} & w_{oh22} & w_{oh23} \\\\\n",
    "\\end{bmatrix}\n",
    "L_{2bias} =\n",
    "\\begin{bmatrix}\n",
    "    w_{o1} \\\\\n",
    "    w_{o2} \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $L_1$ holds the weights between the input layer and the hidden layer,\n",
    "$L_{1bias}$ are the bias terms added to the hidden layer,\n",
    "$L_2$ the weights between the hidden and output layers,\n",
    "and $_{2bias}$ the bias terms added to the output layer.\n",
    "The indexes on the weights summarize with connection the weight apply to.\n",
    "For example, $w_{hi32}$ means the weight on the connection between the 2nd\n",
    "node in the input layer (the $2$ is in the same position as the $i$in the subscript)\n",
    "and the 3rd node in the hidden layer.  The bias terms have only one\n",
    "subscript since they are applied to a single layer only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to execute a forward pass through the described network\n",
    "we right multiply the input vector (call it $\\vec{x}$) with the\n",
    "layer matrix, add the bias and pass it through the activation\n",
    "function (call it $f$).  For this network the forward pass is then:\n",
    "\n",
    "$$\n",
    "\\vec{x} =\n",
    "\\begin{bmatrix}\n",
    "    x_{0} \\\\\n",
    "    x_{1} \\\\\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "\\hat{y} =\n",
    "f\\left(\n",
    "\\begin{bmatrix}\n",
    "    w_{oh11} & w_{oh12} & w_{oh13} \\\\\n",
    "    w_{oh21} & w_{oh22} & w_{oh23} \\\\\n",
    "\\end{bmatrix}\n",
    "f\\left(\n",
    "\\begin{bmatrix}\n",
    "    w_{hi11} & w_{hi12} \\\\\n",
    "    w_{hi21} & w_{hi22} \\\\\n",
    "    w_{hi31} & w_{hi32} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    x_{0} \\\\\n",
    "    x_{1} \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "    w_{h1} \\\\\n",
    "    w_{h2} \\\\\n",
    "    w_{h3} \\\\\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "    w_{o1} \\\\\n",
    "    w_{o2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reduce this equation to a simpler form if we substitute\n",
    "the matrix names we defined above:\n",
    "\n",
    "$$\n",
    "\\hat{y} = f(L_2 f(L_1 \\vec{x} + L_{1bias}) + L_{2bias})\n",
    "$$\n",
    "\n",
    "OK, this is simple enough to reason about.\n",
    "In the same fashion, we can now add more layer to our network.\n",
    "For example, with three layers:\n",
    "\n",
    "$$\n",
    "\\hat{y} = f(L_3 f(L_2 f(L_1 \\vec{x} + L_{1bias}) + L_{2bias}) + L_{3bias})\n",
    "$$\n",
    "\n",
    "More layers follow by left multiplying the matrix,\n",
    "adding the bias and applying the activation function of the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Division\n",
    "\n",
    "In college we learn a procedure to divide one polynomial by another.\n",
    "The procedure consists of adding multiples of the divisor to the dividend\n",
    "in order to remove the highest order term of the dividend.\n",
    "Once the dividend's order is smaller than the divisor the procedure stops,\n",
    "what remains from the dividend is the rest and the sum of the multiples\n",
    "of the divisor is the quotient.\n",
    "\n",
    "We are only interested in a subset of this procedure,\n",
    "namely the division of a polynomial by a polynomial in the form $(x - r)$.\n",
    "If the remainder of a division of a polynomial by a polynomial\n",
    "in the form $(x - r)$ is zero, this means that $r$ is one of the real\n",
    "roots of the divided polynomial.\n",
    "This also means that $(x - r)$ is one of the factors of the polynomial,\n",
    "i.e. terms to multiply together to reach the polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, given the following polynomial:\n",
    "\n",
    "$$\n",
    "x^3 + 2x^2 - 5x - 6\n",
    "$$\n",
    "\n",
    "We can divide it by $(x + 1)$, $(x - 2)$ and $(x + 3)$; since the roots\n",
    "of this polynomial are $-3$, $-1$ and $2$.\n",
    "First let's prove to ourselves that these are the roots of the polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x**3 + 2*x**2 - 5*x - 6\n",
    "\n",
    "print(f(-3))\n",
    "print(f(-1))\n",
    "print(f(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will divide\n",
    "\n",
    "$$\n",
    "\\begin{array}{r|cccc}\n",
    "        & - x^2 & -  x   & + 6  &     \\\\\n",
    "\\hline\n",
    "(x + 1) &   x^3 & + 2x^2 & - 5x & - 6 \\\\ \n",
    "      + & - x^3 & -  x^2 &      &     \\\\ \n",
    "\\hline\n",
    "        &       &    x^2 & - 5x & - 6 \\\\ \n",
    "      + &       & -  x^2 & -  x &     \\\\ \n",
    "\\hline\n",
    "        &       &        & - 6x & - 6 \\\\ \n",
    "      + &       &        &   6x & + 6 \\\\ \n",
    "\\hline\n",
    "        &       &        &      &   0 \\\\ \n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{r|cccc}\n",
    "    &   1 &   2         & - 5 & - 6 \\\\ \n",
    "\\hline\n",
    "- 1 &   1 & (2 + 1(-1))  & (-5 + (-1)(-6)) & (-6 + (-1)(-6)) \\\\ \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{r|cccc}\n",
    "    &   1 &   2 & - 5 & - 6 \\\\ \n",
    "\\hline\n",
    "    & (1 + (0)(-1)) & (2 + (1)(-1))  & (-5 + (1)(-1)) & (-6 + (-6)(-1)) \\\\ \n",
    "- 1 &             1 &              1 &            - 6 &               0 \\\\ \n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generalize the syntetic division.\n",
    "\n",
    "$$\n",
    "w_nx^n + \\cdots + w_3x^3 + w_2x^2 + w_1x^1 + w_0x^0\n",
    "$$\n",
    "\n",
    "The syntetic division will look like:\n",
    "\n",
    "$$\n",
    "\\begin{array}{r|cccccc}\n",
    "  &          w_3 &            w_2 & w_1 & w_0 \\\\ \n",
    "\\hline\n",
    "  & (w_3 + y_4 x) & (w_2 + y_3 x) & (w_1 + y_2 x) & (w_0 + y_1 x) \\\\ \n",
    "x &           y_3 &           y_2 &           y_1 &           y_0 \\\\ \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_4 &= 0 \\\\\n",
    "y_5 &= 0 \\\\\n",
    "y_4 &= w_4 + y_5 x = 0 \\\\\n",
    "y_3 &= w_3 + y_4 x = w_3 \\\\\n",
    "y_2 &= w_2 + y_3 x \\\\\n",
    "y_1 &= w_1 + y_2 x \\\\\n",
    "y_0 &= w_0 + y_1 x \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_0 &= w_0 + (w_1 + y_2 x) x \\\\\n",
    "    &= w_0 + (w_1 + (w_2 + y_3 x) x) x \\\\\n",
    "    &= w_0 + (w_1 + (w_2 + w_3 x) x) x \\\\\n",
    "    &= w_0 + (w_1 + w_2 x + w_3 x^2) x \\\\\n",
    "    &= w_0 + w_1 x + w_2 x^2 + w_3 x^3 = y \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= w_0 + (w_1 + (w_2 + w_3 x) x) x \\\\\n",
    "y &= x (x (w_3 x + w_2) + w_1) + w_0 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And\n",
    "\n",
    "$$\n",
    "\\hat{y} = f(L_3 f(L_2 f(L_1 \\vec{x} + L_{1bias}) + L_{2bias}) + L_{3bias})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y} &= L_3 (L_2 (x (0 + L_{0bias}) + L_{1bias}) + L_{2bias}) + L_{3bias} \\\\\n",
    "\\hat{y} &= x (x (x (0 + L_{0bias}) + L_{1bias}) + L_{2bias}) + L_{3bias} \\\\\n",
    "y       &= x (x (x (0 + w_3) + w_2) + w_1) + w_0 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In summary, a neural network with:\n",
    "\n",
    "- Single node per layer\n",
    "- Input zero\n",
    "- Bias on on input layer\n",
    "- All weights between nodes set to $x$\n",
    "- All activation functions as identity\n",
    "\n",
    "Performs the evaluation of a polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we can optimize the bias terms at each layer.\n",
    "For this we need a loss function, say:\n",
    "\n",
    "$$\n",
    "E = (y - \\hat{y})^2\n",
    "$$\n",
    "\n",
    "All gradients can be easily precomputed, since:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{\\hat{y}}} = 2(\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "We have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial{E}}{\\partial{L_{3bias}}} &= 2(\\hat{y} - y) \\\\\n",
    "\\frac{\\partial{E}}{\\partial{L_{2bias}}} &= 2x(\\hat{y} - y) \\\\\n",
    "\\frac{\\partial{E}}{\\partial{L_{1bias}}} &= 2x^2(\\hat{y} - y) \\\\\n",
    "\\frac{\\partial{E}}{\\partial{L_{0bias}}} &= 2x^3(\\hat{y} - y) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have been using $x$ and $y$ as scalars,\n",
    "yet nothing prevents us from using min-batch vectors instead.\n",
    "The difference in that case is that we need to change the loss\n",
    "function and derivatives as follows:\n",
    "\n",
    "$$\n",
    "N = \\tt{number\\ of\\ items\\ in\\ mini-batch} \\\\\n",
    "E = \\tt{mean} \\left( \\left( y - \\hat{y} \\right)^2 \\right) \\\\\n",
    "\\frac{\\partial{E}}{\\partial{\\hat{y}}} = \\frac{2(\\hat{y} - y)}{N} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
